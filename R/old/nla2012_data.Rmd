---
title: nla2012_data
author: Bryan
date: February 21, 2017
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

options(stringsAsFactors = FALSE) 
library(knitr)
library(tidyverse)


#location of data files
kali<-'Y:/data/NLA2012/raw_data/'

```
## To Do
* Check that the metadata for "phytocnt" are in the metadata document for "phytotaxa"
* convert all variable names in the metadata and field to snake_case
* no description for "SAM_CODE" in the metadata record for waterchem

## Naming Conventions
* All variable names in lowercase
* use snake case for compound names (e.g., big_data)


## Introduction

The National Lake Assessment data are available online at: https://www.epa.gov/national-aquatic-resource-surveys/data-national-aquatic-resource-surveys.  The data include 20 data files and 19 metadata files (according to the website two of the datasets share metadata).  All files were downloaded to our server Kali.  This server has limited access so if you want the data you use the following script to download and save the data: https://github.com/willbmisled/lakes_database/blob/master/R/download_data_from_nla.Rmd.  

NOTE: some of the original metadata files are in tab delimited format.  This script changes them to csv format.  So some of the names (the extensions) will change. A field called dataset with an abbreviated name based on the filename was added to more easily track the datasets.  Both the data and the metadata for a sampling effort will have the same dataset but different filenames.


The downloaded files with their original, new names, and shortnames are shown below:


```{r kable, include=TRUE, echo=FALSE}
datasets<-read.csv('../data/NLA2012_data_sources.csv',sep=',')

#add a shorTname to a subset of the datasets file.
datasets$dataset<-as.vector(sapply(datasets$filename,function(x) unlist(strsplit(x,"_"))[2]))
datasets$dataset[grep('nla2012_wide',datasets$filename)]<-as.vector(sapply(datasets$filename[grep('nla2012_wide',datasets$filename)],function(x)
    unlist(strsplit(x,"_"))[3]))
datasets$dataset[grep('nla_2012',datasets$filename)]<-"condition"
datasets$dataset[grep('bentmet.csv',datasets$filename)]<-"bentmet"
datasets$indicator<-gsub(" ","_",tolower(datasets$indicator))

#change order
raw<-datasets[,c(1:3,8,4:6)]
write.table(raw,'../data/raw_data/nla2012_raw_data_sources.csv',row.names=F,sep=',')

kable(datasets[,c(4,5,8)])
```

###Make a master file of the data definitions

* The NLA data are in 20 flat files
* Each flat file has a combination of unique fields and some fields having to do with sampling, location or logistics.
* The sampling, location, and logistics fields are repeated across datasets.
* GOAL: to combine all the metadata files into a single file and to remove redundancies.
* First problem, column_names are not the same for all metadata files.  
    - The variable name is called "PARAMETER", "VARIABLE", or "DATA_COLUMN" depending on the metadata file
      - For all files this was changed to "field"
    - Most metadata files have a column called "LABEL" that defines the "field" but some have this information in two columns; one called "LABEL" and the other "DESCRIPTION".  Fortunately for each line only one or the other is used.
      - The data from both columns are combined and named "description"
* Decide which field to keep. The fields to delete are:
    - "TABLE_NAME": Not in all meta data records; this seems to be a shorthand for the datasource
    - "SAMPLE_TYPE": This seems to be a shorthand for the datasource; may relate to table name in the original database used to derive the flat files.
    - "LEGAL_VALUES": for categorical data these are the acceptable values.  Not consistent across datasets so it will be eliminated.  Refer to the original metadata if this is of interest.
    - "RANGE_HIGH": probably for QA. Appears to be the highest acceptable values for the field.  Not consistent across datasets so it will be eliminated.  Refer to the original metadata if this is of interest.
    - "RANGE_LOW": probably for QA. Appears to be the lowest acceptable values for the field.  Not consistent across datasets so it will be eliminated.  Refer to the original metadata if this is of interest.
* Field to keep:
    
   
   
   
    - Rename name="PARAMETER", name="VARIABLE" or name="DATA_COLUMN" to name="field"
    - Rename name="LABEL" to name="DESCRIPTION"
    - delete column TABLE_NAME when it occurs


```{r harvest, include=TRUE, echo=FALSE}
m<-filter(datasets,type=='metadata')[-8,] %>%
    select(dataset,filename,dataset)

meta<-c() #empty space to hold the data
for(i in c(1:nrow(m))){
#for(i in c(1:4)){
#read the data
  a<-read.csv(paste(kali,m$filename[i],sep=''),sep=',',na.strings = c(""," "))
#rename some column_names (if they exist) 
  names(a)[names(a)%in%c("PARAMETER","VARIABLE")]<-"DATA_COLUMN" 
#delete field "TABLE_NAME"  
  a<-select(a, -matches("TABLE_NAME"))
#add filename
  #a$dataset<-m$dataset[i]
  #a$filename<-m$filename[i]
  a$dataset<-m$dataset[i]
  #a$order=i
#add to meta
  meta<-bind_rows(meta,a)
}

#combine DESCRIPTION and LABEL then drop LABEL
meta$DESCRIPTION<-ifelse(is.na(meta$LABEL),meta$DESCRIPTION,meta$LABEL)
meta<-select(meta, -matches("LABEL"))

#rename 'DATA_COLUMN' to 'field'
#meta<-rename(meta,field=DATA_COLUMN)
#rename 'DESCRIPTION' to 'description'
#meta<-rename(meta,description=DESCRIPTION)

meta<-select(meta,dataset,field=DATA_COLUMN,units=UNITS,description=DESCRIPTION)
```

* Check the field and DESCRIPTION fields for redundancy
    - many of the description fields are used for multiple column_names; mostly in the phab data; fine as is.
    - some of the DATA_COLUMN have slight variations in the descriptions.  We will choose one description for each DATA_COLUMN value.
    - YCOORD has the wrong information.  We can recover the correct description from "XCOORD"
* Check the units fields for inconsistencies and redundancy
    - Change "M" to "m"
    - Change "NA", "None", "NONE" to NA



```{r newDesc, include=FALSE, echo=FALSE}
#remove leading and trailing spaces in the DESCRIPTION
  meta$description<-trimws(meta$description)

#check lengths and look for redundancy in the column names and descriptions
length(meta$field) #1926
length(unique(meta$field)) #1716
length(unique(meta$description)) #1655

chk<-distinct(select(meta, field, description)) 
nrow(chk) #1726

des<-as.data.frame(table(chk$description))
rep<-filter(des,Freq>1)[,1]
temp<-arrange(filter(chk,description%in%rep),description)
#write.table(temp, file='../data/temp.csv',row.names=F,sep=',')

col<-as.data.frame(table(chk$field))
rep1<-filter(col,Freq>1)[,1]
temp1<-arrange(filter(chk,field%in%rep1),field)
#write.table(temp1, file='../data/temp1.csv',row.names=F,sep=',')

#update the descriptions to eliminate redundancy
newDesc<-data.frame(field=c('DEPTH','DUPLICATE_DO','FFG','LAB','ODOR','ODOR_DESC','REVIEWED_BY_INITIAL','TAXA_ID','TOTALHG_RESULT','UID','YCOORD'),new=c('Depth at collection point','Duplicate DO reading taken','Functional feeding group; codes separated by commas if more than one','Lab name','Odor present','Description of odor','Intials of form reviewer','Taxa identification number','Analyte value for sediment mercury for Total Mercury','Unique identifier for sampling event','y-coordinate from US Contiguous Albers Equal Area Conic projection EXCEPT Hawaii sites which are calculated from Hawaii Albers Equal Area Conic projection'))

a<-full_join(meta,newDesc)
a$description[!is.na(a$new)]<-a$new[!is.na(a$new)]
meta<-select(a,-new)

#check lengths and look for redundancy in the column names and descriptions
length(meta$field) #1926 unchanged
length(unique(meta$field)) #1716 unchanged
length(unique(meta$description)) #1655 change to 1646

nrow(distinct(select(meta, field, description))) #1726 change to 1716

#check for inconsistencies in the field "units"
meta$units<-trimws(meta$units)
arrange(distinct(select(meta, units)),units)

#make some changes for consistency
  #replace "M" with "m" (meters)
meta<-mutate(meta,units=ifelse(units=='M','m',units))

meta<-mutate(meta,units=ifelse(units=='None' | units=='NONE' | units=='NA',NA,units))

#check for multiple values per field
chk<-distinct(select(meta, field, units)) 
nrow(chk) #1716
des<-as.data.frame(table(chk$field))
rep<-filter(des,Freq>1)[,1]  #none
```

* Create the meta_wide field-a crosstab showing the fields(rows) and the datasets (columns)

```{r spread, include=FALSE, echo=FALSE}
#create pivot table of field(rows) by dataset(columns)
a<-spread(as.data.frame(table(meta$field,meta$dataset)),Var2,Freq)%>%
    rename(field=Var1)

#join values of units and description 
#get unique vales for field, units, and description
b<-distinct(select(meta, field, units,description))
#join to pivot
meta_wide<-left_join(b,a)

#data dictionaries

#write the final files
#write.table(meta_long, file='../data/meta_long.csv',row.names=F,sep=',')
#write.table(meta_wide, file='../data/meta_wide.csv',row.names=F,sep=',')
#write.table(datasets, file='../data/datasets.csv',row.names=F,sep=',')
#save(meta_long,meta_wide,datasets,file='../data/meta2012.rda')
```

